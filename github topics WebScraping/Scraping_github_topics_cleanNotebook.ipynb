{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc9602c6",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4135fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2897da25",
   "metadata": {},
   "source": [
    "# scrape topics \n",
    "## get username, repo name, repo url and stars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a4116c",
   "metadata": {},
   "source": [
    "- extract username, repo_name, repo_url and stars from a given tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec04158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(repo_tag, star_tag):\n",
    "    base_url = \"https://github.com\"\n",
    "    atags = repo_tag.find_all('a')\n",
    "    username = atags[0].text.strip()\n",
    "    repo_name = atags[1].text.strip()\n",
    "    repo_url = base_url + atags[1]['href']\n",
    "    stars = parse_star_tag(star_tag.text.strip())\n",
    "    return username, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3723eba",
   "metadata": {},
   "source": [
    "- parse stars from \"8K\" to 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e5a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_tag(star):\n",
    "    if star[-1]=='k':\n",
    "        return int(float(star[:-1])*1000)\n",
    "    else:\n",
    "        return int(star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162a6f1",
   "metadata": {},
   "source": [
    "- create a data frame that contains\n",
    "- username | repo_name | stars | repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a710c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_repo_info_df(repo_tags, star_tags):\n",
    "    topic_repos_dict ={\n",
    "                        'username':[],\n",
    "                        'repo_name':[],\n",
    "                        'stars':[],\n",
    "                        'repo_url':[]\n",
    "                    }\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "    return topic_repos_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf87b3a3",
   "metadata": {},
   "source": [
    "- scrape the url and get the required tags\n",
    "- use functions above to parse stars and create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80d60cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_url):\n",
    "    #download the page\n",
    "    response = requests.get(topic_url)\n",
    "    #check response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to load page {}\").format(topic_url)\n",
    "    #parse using Beautiful soup\n",
    "    topic_doc =  BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # get h2 tags containing repo title, url and username\n",
    "    h1_class = \"f3 color-fg-muted text-normal lh-condensed\"\n",
    "    repo_tags = topic_doc.find_all('h3', class_=h1_class)\n",
    "    # get stars tags\n",
    "    star_class = 'repo-stars-counter-star'\n",
    "    star_tags = topic_doc.find_all('span', {'id':star_class})\n",
    "\n",
    "    topic_repos_dict = create_repo_info_df(repo_tags, star_tags)\n",
    "        \n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2553f05",
   "metadata": {},
   "source": [
    "# For every topic create a csv with topic name, description and url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54101f27",
   "metadata": {},
   "source": [
    "- given topic html extract titles, description and urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ea65223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class = \"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
    "    topic_title_p_tags = doc.find_all('p', \n",
    "                          {'class': selection_class})\n",
    "    topic_titles = [ tag.text for tag in topic_title_p_tags]\n",
    "    return topic_titles\n",
    "\n",
    "def get_topic_desc(doc):\n",
    "    selection_class = \"f5 color-fg-muted mb-0 mt-1\"\n",
    "    topic_desc_p_tags = doc.find_all('p', class_=selection_class)\n",
    "    topic_desc = [ tag.text.strip() for tag in topic_desc_p_tags]\n",
    "    return topic_desc\n",
    "\n",
    "def get_topic_urls(doc):\n",
    "    selection_class = \"no-underline flex-1 d-flex flex-column\"\n",
    "    url_a_tag = doc.find_all('a', class_=selection_class)\n",
    "    base_url = \"https://github.com\"\n",
    "    topic_urls = [base_url+url['href'] for url in url_a_tag]\n",
    "    return topic_urls\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56004e",
   "metadata": {},
   "source": [
    "- loop through topic and scrape titles, description, urls \n",
    "- create data frame that contains \n",
    "- title | description | url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3ade5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topic_url = 'https://github.com/topics'\n",
    "    #download the page\n",
    "    response = requests.get(topic_url)\n",
    "    #check response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Failed to load page {}\".format(topic_url))\n",
    "    base_url = \"https://github.com\"\n",
    "    page_content = response.text\n",
    "    \n",
    "    doc = BeautifulSoup(page_content, 'html.parser')\n",
    "    \n",
    "\n",
    "    topic_titles = get_topic_titles(doc)\n",
    "    topic_desc = get_topic_desc(doc)\n",
    "    topic_urls = get_topic_urls(doc)\n",
    "    \n",
    "    topic_dict = {\"title\":topic_titles, \n",
    "                  \"description\":topic_desc,\n",
    "                  \"url\": topic_urls}\n",
    "\n",
    "    topics_df = pd.DataFrame(topic_dict)\n",
    "    return topics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ba3b1",
   "metadata": {},
   "source": [
    "# bring it together "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b974cbe",
   "metadata": {},
   "source": [
    "- create csv of the scraped topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48825dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url, topic_name):\n",
    "    topic_repo_df = get_topic_repos(topic_url)\n",
    "    fname = \"github_scrape/\" + topic_name + '.csv'\n",
    "    \n",
    "    if os.path.exists(fname):\n",
    "        print(\"The file {} already exists. Skipping..\".format(fname))\n",
    "        return\n",
    "    \n",
    "    topic_repo_df.to_csv(fname, index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c3388",
   "metadata": {},
   "source": [
    "- loop through all topics and use funvtions above to create the csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "305338f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print('Scraping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    \n",
    "    os.makedirs('github_scrape', exist_ok=True)\n",
    "    \n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'], row['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a379c9d",
   "metadata": {},
   "source": [
    "# call the function for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "208778ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list of topics\n",
      "scraping top repositories for \"3D\"\n",
      "The file github_scrape/3D.csv already exists. Skipping..\n",
      "scraping top repositories for \"Ajax\"\n",
      "The file github_scrape/Ajax.csv already exists. Skipping..\n",
      "scraping top repositories for \"Algorithm\"\n",
      "The file github_scrape/Algorithm.csv already exists. Skipping..\n",
      "scraping top repositories for \"Amp\"\n",
      "The file github_scrape/Amp.csv already exists. Skipping..\n",
      "scraping top repositories for \"Android\"\n",
      "The file github_scrape/Android.csv already exists. Skipping..\n",
      "scraping top repositories for \"Angular\"\n",
      "The file github_scrape/Angular.csv already exists. Skipping..\n",
      "scraping top repositories for \"Ansible\"\n",
      "The file github_scrape/Ansible.csv already exists. Skipping..\n",
      "scraping top repositories for \"API\"\n",
      "The file github_scrape/API.csv already exists. Skipping..\n",
      "scraping top repositories for \"Arduino\"\n",
      "The file github_scrape/Arduino.csv already exists. Skipping..\n",
      "scraping top repositories for \"ASP.NET\"\n",
      "The file github_scrape/ASP.NET.csv already exists. Skipping..\n",
      "scraping top repositories for \"Atom\"\n",
      "The file github_scrape/Atom.csv already exists. Skipping..\n",
      "scraping top repositories for \"Awesome Lists\"\n",
      "The file github_scrape/Awesome Lists.csv already exists. Skipping..\n",
      "scraping top repositories for \"Amazon Web Services\"\n",
      "The file github_scrape/Amazon Web Services.csv already exists. Skipping..\n",
      "scraping top repositories for \"Azure\"\n",
      "The file github_scrape/Azure.csv already exists. Skipping..\n",
      "scraping top repositories for \"Babel\"\n",
      "The file github_scrape/Babel.csv already exists. Skipping..\n",
      "scraping top repositories for \"Bash\"\n",
      "The file github_scrape/Bash.csv already exists. Skipping..\n",
      "scraping top repositories for \"Bitcoin\"\n",
      "The file github_scrape/Bitcoin.csv already exists. Skipping..\n",
      "scraping top repositories for \"Bootstrap\"\n",
      "The file github_scrape/Bootstrap.csv already exists. Skipping..\n",
      "scraping top repositories for \"Bot\"\n",
      "The file github_scrape/Bot.csv already exists. Skipping..\n",
      "scraping top repositories for \"C\"\n",
      "The file github_scrape/C.csv already exists. Skipping..\n",
      "scraping top repositories for \"Chrome\"\n",
      "The file github_scrape/Chrome.csv already exists. Skipping..\n",
      "scraping top repositories for \"Chrome extension\"\n",
      "The file github_scrape/Chrome extension.csv already exists. Skipping..\n",
      "scraping top repositories for \"Command line interface\"\n",
      "The file github_scrape/Command line interface.csv already exists. Skipping..\n",
      "scraping top repositories for \"Clojure\"\n",
      "The file github_scrape/Clojure.csv already exists. Skipping..\n",
      "scraping top repositories for \"Code quality\"\n",
      "The file github_scrape/Code quality.csv already exists. Skipping..\n",
      "scraping top repositories for \"Code review\"\n",
      "The file github_scrape/Code review.csv already exists. Skipping..\n",
      "scraping top repositories for \"Compiler\"\n",
      "The file github_scrape/Compiler.csv already exists. Skipping..\n",
      "scraping top repositories for \"Continuous integration\"\n",
      "The file github_scrape/Continuous integration.csv already exists. Skipping..\n",
      "scraping top repositories for \"COVID-19\"\n",
      "The file github_scrape/COVID-19.csv already exists. Skipping..\n",
      "scraping top repositories for \"C++\"\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985170b",
   "metadata": {},
   "source": [
    "## future work\n",
    "### get more topics by looping through\n",
    "### https://github.com/topics?page= + i -> i from 1 to 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f535f872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
